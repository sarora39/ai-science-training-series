2024-04-08 04:11:19,206 INFO:   Effective batch size is 512.
2024-04-08 04:11:19,231 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in "model_dir_bert_large_pytorch" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.
2024-04-08 04:11:19,232 INFO:   No checkpoints were found in "model_dir_bert_large_pytorch".
2024-04-08 04:11:19,232 INFO:   No checkpoint was provided. Using randomly initialized model parameters.
2024-04-08 04:11:20,497 INFO:   Saving checkpoint at step 0
2024-04-08 04:11:47,814 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_0.mdl
2024-04-08 04:12:01,908 INFO:   Compiling the model. This may take a few minutes.
2024-04-08 04:12:01,910 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-08 04:12:03,273 INFO:   Initiating a new image build job against the cluster server.
2024-04-08 04:12:03,387 INFO:   Custom worker image build is disabled from server.
2024-04-08 04:12:03,395 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-08 04:12:03,737 INFO:   Initiating a new compile wsjob against the cluster server.
2024-04-08 04:12:03,860 INFO:   compile job id: wsjob-nwjyttgpfktvyh4kpmmz5n, remote log path: /n1/wsjob/workdir/job-operator/wsjob-nwjyttgpfktvyh4kpmmz5n
2024-04-08 04:12:13,909 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-08 04:12:43,912 INFO:   Poll ingress status: Waiting for job ingress readiness.
2024-04-08 04:12:53,926 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-08 04:12:57,890 INFO:   Pre-optimization transforms...
2024-04-08 04:13:03,831 INFO:   Optimizing layouts and memory usage...
2024-04-08 04:13:03,875 INFO:   Gradient accumulation enabled
2024-04-08 04:13:03,876 WARNING:   Gradient accumulation will search for an optimal micro batch size based on internal performance models, which can lead to an increased compile time. Specify `micro_batch_size` option in the 'train_input/eval_input' section of your .yaml parameter file to set the gradient accumulation microbatch size, if an optimal microbatch size is known.

2024-04-08 04:13:03,879 INFO:   Gradient accumulation trying sub-batch size 8...
2024-04-08 04:13:08,982 INFO:   Exploring floorplans
2024-04-08 04:13:15,700 INFO:   Exploring data layouts
2024-04-08 04:13:28,309 INFO:   Optimizing memory usage
2024-04-08 04:14:16,435 INFO:   Gradient accumulation trying sub-batch size 64...
2024-04-08 04:14:21,937 INFO:   Exploring floorplans
2024-04-08 04:14:31,259 INFO:   Exploring data layouts
2024-04-08 04:14:50,337 INFO:   Optimizing memory usage
2024-04-08 04:15:22,781 INFO:   Gradient accumulation trying sub-batch size 32...
2024-04-08 04:15:28,204 INFO:   Exploring floorplans
2024-04-08 04:15:35,271 INFO:   Exploring data layouts
2024-04-08 04:15:50,519 INFO:   Optimizing memory usage
2024-04-08 04:16:26,550 INFO:   Gradient accumulation trying sub-batch size 128...
2024-04-08 04:16:32,767 INFO:   Exploring floorplans
2024-04-08 04:16:43,622 INFO:   Exploring data layouts
2024-04-08 04:17:02,399 INFO:   Optimizing memory usage
2024-04-08 04:17:30,022 INFO:   Gradient accumulation trying sub-batch size 256...
2024-04-08 04:17:36,764 INFO:   Exploring floorplans
2024-04-08 04:17:53,290 INFO:   Exploring data layouts
2024-04-08 04:18:20,244 INFO:   Optimizing memory usage
2024-04-08 04:19:05,947 INFO:   Exploring floorplans
2024-04-08 04:19:09,954 INFO:   Exploring data layouts
2024-04-08 04:19:45,268 INFO:   Optimizing memory usage
2024-04-08 04:20:21,755 INFO:   No benefit from gradient accumulation expected. Compile will proceed at original per-box batch size 512 with 6 lanes

2024-04-08 04:20:21,812 INFO:   Post-layout optimizations...
2024-04-08 04:20:34,937 INFO:   Allocating buffers...
2024-04-08 04:20:37,519 INFO:   Code generation...
2024-04-08 04:20:51,206 INFO:   Compiling image...
2024-04-08 04:20:51,212 INFO:   Compiling kernels
2024-04-08 04:24:15,355 INFO:   Compiling final image
2024-04-08 04:27:05,415 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_8939750200954608837
2024-04-08 04:27:05,484 INFO:   Heartbeat thread stopped for wsjob-nwjyttgpfktvyh4kpmmz5n.
2024-04-08 04:27:05,510 INFO:   Compile was successful!
2024-04-08 04:27:05,567 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.
2024-04-08 04:27:14,234 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-08 04:27:15,681 INFO:   Initiating a new execute wsjob against the cluster server.
2024-04-08 04:27:16,449 INFO:   execute job id: wsjob-fhinsqdwzqebojjzgbkwgn, remote log path: /n1/wsjob/workdir/job-operator/wsjob-fhinsqdwzqebojjzgbkwgn
2024-04-08 04:27:26,707 INFO:   Poll ingress status: Waiting for job running, current job status: Scheduled, msg: job is scheduled. 
2024-04-08 04:27:36,489 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-08 04:27:56,541 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-08 04:27:57,396 INFO:   Preparing to execute using 1 CSX
2024-04-08 04:28:26,190 INFO:   About to send initial weights
2024-04-08 04:30:15,969 INFO:   Finished sending initial weights
2024-04-08 04:30:15,989 INFO:   Finalizing appliance staging for the run
2024-04-08 04:30:16,027 INFO:   Waiting for device programming to complete
2024-04-08 04:31:00,441 INFO:   Device programming is complete
2024-04-08 04:31:01,241 INFO:   Using network type: ROCE
2024-04-08 04:31:01,261 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...
2024-04-08 04:31:01,277 INFO:   Input workers have begun streaming input data
2024-04-08 04:31:17,958 INFO:   Appliance staging is complete
2024-04-08 04:31:18,020 INFO:   Beginning appliance run
2024-04-08 04:32:54,298 INFO:   | Train Device=CSX, Step=100, Loss=9.39062, Rate=535.43 samples/sec, GlobalRate=535.43 samples/sec
2024-04-08 04:33:37,086 INFO:   | Train Device=CSX, Step=200, Loss=8.70312, Rate=932.07 samples/sec, GlobalRate=739.80 samples/sec
2024-04-08 04:34:04,249 INFO:   | Train Device=CSX, Step=300, Loss=7.79688, Rate=1503.74 samples/sec, GlobalRate=927.65 samples/sec
2024-04-08 04:34:47,205 INFO:   | Train Device=CSX, Step=400, Loss=7.39062, Rate=1316.66 samples/sec, GlobalRate=982.09 samples/sec
2024-04-08 04:35:35,435 INFO:   | Train Device=CSX, Step=500, Loss=7.80469, Rate=1163.62 samples/sec, GlobalRate=997.02 samples/sec
2024-04-08 04:36:04,717 INFO:   | Train Device=CSX, Step=600, Loss=7.53125, Rate=1514.55 samples/sec, GlobalRate=1073.95 samples/sec
2024-04-08 04:36:33,476 INFO:   | Train Device=CSX, Step=700, Loss=7.35156, Rate=1673.98 samples/sec, GlobalRate=1138.48 samples/sec
2024-04-08 04:37:02,444 INFO:   | Train Device=CSX, Step=800, Loss=7.27344, Rate=1730.11 samples/sec, GlobalRate=1191.48 samples/sec
2024-04-08 04:37:37,006 INFO:   | Train Device=CSX, Step=900, Loss=7.35938, Rate=1580.88 samples/sec, GlobalRate=1217.97 samples/sec
2024-04-08 04:38:25,489 INFO:   | Train Device=CSX, Step=1000, Loss=7.12500, Rate=1265.96 samples/sec, GlobalRate=1199.57 samples/sec
2024-04-08 04:38:25,491 INFO:   Saving checkpoint at step 1000
2024-04-08 04:43:56,141 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_1000.mdl
2024-04-08 04:44:26,294 INFO:   Heartbeat thread stopped for wsjob-fhinsqdwzqebojjzgbkwgn.
2024-04-08 04:44:26,484 INFO:   Training completed successfully!
2024-04-08 04:44:26,488 INFO:   Processed 512000 sample(s) in 426.819169997 seconds.
